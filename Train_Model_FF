name: Train New FFN Model for FF 
description: Compiles and trains the FFN model using the training dataset and saves the trained model.
inputs:
  - {name: ff_model, type: Model}
  - {name: train_dataset, type: Dataset}
outputs:
  - {name: trained_model, type: Model}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet tensorflow keras || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet tensorflow keras --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import tensorflow as tf
        import keras
        from keras import ops
        from keras.optimizers import Adam
        from tensorflow.compiler.tf2xla.python import xla
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--ff_model', type=str, required=True)
        parser.add_argument('--train_dataset', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        args = parser.parse_args()
        
        # Define the model classes
        @keras.saving.register_keras_serializable()
        class FFDense(keras.layers.Layer):
            def __init__(self, units, init_optimizer, loss_metric, num_epochs=50,
                         use_bias=True, kernel_initializer="glorot_uniform",
                         bias_initializer="zeros", kernel_regularizer=None,
                         bias_regularizer=None, **kwargs):
                super().__init__(**kwargs)
                self.dense = keras.layers.Dense(
                                                units,
                                                activation=None,
                                                use_bias=use_bias,
                                                kernel_initializer=kernel_initializer,
                                                bias_initializer=bias_initializer,
                                                kernel_regularizer=kernel_regularizer,
                                                bias_regularizer=bias_regularizer
                                                )
                self.relu = keras.layers.ReLU()
                self.optimizer = init_optimizer()
                self.loss_metric = loss_metric
                self.threshold = 1.5
                self.num_epochs = num_epochs

            def call(self, x):
                x_norm = ops.norm(x, ord=2, axis=1, keepdims=True) + 1e-4
                x_dir = x / x_norm
                res = self.dense(x_dir)
                return self.relu(res)

            def forward_forward(self, x_pos, x_neg):
                for _ in range(self.num_epochs):
                    with tf.GradientTape() as tape:
                        g_pos = ops.mean(ops.power(self.call(x_pos), 2), 1)
                        g_neg = ops.mean(ops.power(self.call(x_neg), 2), 1)
                        loss = ops.log(1 + ops.exp(ops.concatenate([-g_pos + self.threshold,
                                                                    g_neg - self.threshold], 0)))
                        mean_loss = ops.cast(ops.mean(loss), dtype="float32")
                        self.loss_metric.update_state([mean_loss])
                    grads = tape.gradient(mean_loss, self.dense.trainable_weights)
                    self.optimizer.apply_gradients(zip(grads, self.dense.trainable_weights))
                return ops.stop_gradient(self.call(x_pos)), ops.stop_gradient(self.call(x_neg)), self.loss_metric.result()

        @keras.saving.register_keras_serializable()
        class FFNetwork(keras.Model):
            def __init__(self, dims, init_layer_optimizer=lambda: keras.optimizers.Adam(0.03), **kwargs):
                super().__init__(**kwargs)
                self.init_layer_optimizer = init_layer_optimizer
                self.loss_var = keras.Variable(0.0, trainable=False, dtype="float32")
                self.loss_count = keras.Variable(0.0, trainable=False, dtype="float32")
                self.layer_list = [keras.Input(shape=(dims[0],))]
                self.metrics_built = False
                for d in range(len(dims) - 1):
                    self.layer_list.append(
                        FFDense(dims[d + 1],
                                init_optimizer=self.init_layer_optimizer,
                                loss_metric=keras.metrics.Mean())
                    )

            @tf.function(reduce_retracing=True)
            def overlay_y_on_x(self, data):
                X_sample, y_sample = data
                max_sample = ops.cast(ops.amax(X_sample, axis=0, keepdims=True), dtype="float64")
                X_zeros = ops.zeros([10], dtype="float64")
                X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])
                X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])
                return X_sample, y_sample

            @tf.function(reduce_retracing=True)
            def predict_one_sample(self, x):
                goodness_per_label = []
                x = ops.reshape(x, [ops.shape(x)[0] * ops.shape(x)[1]])
                for label in range(10):
                    h, _ = self.overlay_y_on_x((x, label))
                    h = ops.reshape(h, [-1, ops.shape(h)[0]])
                    goodness = []
                    for layer in self.layer_list[1:]:
                        h = layer(h)
                        goodness.append(ops.mean(ops.power(h, 2), 1))
                    goodness_per_label.append(ops.expand_dims(ops.sum(goodness, keepdims=True), 1))
                return ops.cast(ops.argmax(tf.concat(goodness_per_label, 1), 1), dtype="float64")

            def update_pierce_params(self, pierce_params):
                self.optimiser = pierce_params.get("optimiser", keras.optimizers.Adam(learning_rate=0.03))
                self.threshold = pierce_params.get("threshold", 1.5)
                for layer in self.layer_list:
                    if hasattr(layer, "optimizer"):
                        layer.optimizer = self.optimiser

            def predict(self, data):
                return ops.vectorized_map(self.predict_one_sample, data).numpy().astype(int)

            @tf.function(jit_compile=False)
            def train_step(self, data):
                x, y = data
                if not self.metrics_built:
                    for metric in self.metrics:
                        if hasattr(metric, "build"):
                            metric.build(y, y)
                    self.metrics_built = True
                x = ops.reshape(x, [-1, ops.shape(x)[1] * ops.shape(x)[2]])
                x_pos, y = ops.vectorized_map(self.overlay_y_on_x, (x, y))
                random_y = tf.random.shuffle(y)
                x_neg, _ = tf.map_fn(self.overlay_y_on_x, (x, random_y))
                h_pos, h_neg = x_pos, x_neg
                for layer in self.layers:
                    if isinstance(layer, FFDense):
                        h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)
                        self.loss_var.assign_add(loss)
                        self.loss_count.assign_add(1.0)
                    else:
                        h_pos = layer(h_pos)
                return {"FinalLoss": self.loss_var / self.loss_count}
        
        print(f"Loading model from {args.ff_model}")
        
        # Load model config and weights
        config_path = args.ff_model + '_config.json'
        weights_path = args.ff_model + '_weights.h5'
        
        with open(config_path, 'r') as f:
            model_config = json.load(f)
        
        # Recreate model
        model = FFNetwork(model_config['dims'])
        
        # Build model with dummy data
        dummy_input = tf.random.normal((1, 784))
        _ = model(dummy_input)
        
        # Load weights
        model.load_weights(weights_path)
        
        print(f"Loaded model weights from {weights_path}")
        # Load dataset
        print(f"Loading train dataset from {args.train_dataset}")
        train_data = tf.data.experimental.load(args.train_dataset)
        # Compile and train
        model.compile(
            optimizer=Adam(learning_rate=0.03),
            loss="mse",
            jit_compile=False,
            metrics=[]
        )
        model.fit(train_data, epochs=2)
        # Save the trained model
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        
        # Save weights and config separately
        trained_weights_path = args.trained_model + '_weights.h5'
        trained_config_path = args.trained_model + '_config.json'
        
        model.save_weights(trained_weights_path)
        with open(trained_config_path, 'w') as f:
            json.dump(model_config, f)
        
        print(f"Saved trained model weights to {trained_weights_path}")
        print(f"Saved trained model config to {trained_config_path}")
        print(f"Saved trained model to {args.trained_model}")
    args:
      - --ff_model
      - {inputPath: ff_model}
      - --train_dataset
      - {inputPath: train_dataset}
      - --trained_model
      - {outputPath: trained_model}
