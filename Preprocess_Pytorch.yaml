name: Preprocess Data (pytorch)
description: Normalizes MNIST data and creates torch datasets for training and testing.
inputs:
  - {name: x_train, type: Dataset}
  - {name: y_train, type: Dataset}
  - {name: x_test, type: Dataset}
  - {name: y_test, type: Dataset}
outputs:
  - {name: train_dataset, type: Dataset}
  - {name: test_dataset, type: Dataset}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch numpy || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch numpy --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import pickle
        import os
        import numpy as np

        parser = argparse.ArgumentParser()
        parser.add_argument('--x_train', type=str, required=True)
        parser.add_argument('--y_train', type=str, required=True)
        parser.add_argument('--x_test', type=str, required=True)
        parser.add_argument('--y_test', type=str, required=True)
        parser.add_argument('--train_dataset', type=str, required=True)
        parser.add_argument('--test_dataset', type=str, required=True)
        args = parser.parse_args()

        # Load data
        with open(args.x_train, "rb") as f: x_train = pickle.load(f)
        with open(args.y_train, "rb") as f: y_train = pickle.load(f)
        with open(args.x_test, "rb") as f: x_test = pickle.load(f)
        with open(args.y_test, "rb") as f: y_test = pickle.load(f)

        # Normalize
        x_train = x_train.astype(np.float32) / 255.0
        x_test = x_test.astype(np.float32) / 255.0
        y_train = y_train.astype(np.int64)
        y_test = y_test.astype(np.int64)

        #Flatten
        if len(x_train.shape) > 2:
            x_train = x_train.reshape(len(x_train), -1)
        if len(x_test.shape) > 2:
            x_test = x_test.reshape(len(x_test), -1)

        
        # Ensure output dirs
        os.makedirs(os.path.dirname(args.train_dataset), exist_ok=True)
        os.makedirs(os.path.dirname(args.test_dataset), exist_ok=True)

        with open(args.train_dataset, "wb") as f:
            pickle.dump((x_train, y_train), f)
        with open(args.test_dataset, "wb") as f:
            pickle.dump((x_test, y_test), f)

    args:
      - --x_train
      - {inputPath: x_train}
      - --y_train
      - {inputPath: y_train}
      - --x_test
      - {inputPath: x_test}
      - --y_test
      - {inputPath: y_test}
      - --train_dataset
      - {outputPath: train_dataset}
      - --test_dataset
      - {outputPath: test_dataset}
