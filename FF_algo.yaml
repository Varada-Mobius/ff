name: Forward-Forward Training

description: Trains a feedforward model one layer at a time using the Forward-Forward algorithm on positive and negative training datasets.

inputs:
  - {name: model, type: Model}
  - {name: pos_train_data, type: Dataset}
  - {name: neg_train_data, type: Dataset}
  - {name: num_epochs, type: Integer}
  - {name: threshold, type: Float}
  - {name: learning_rate, type: Float}

outputs:
  - {name: trained_model, type: Model}

implementation:
  container:
    image: python:3.8
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet tensorflow numpy
        exec python3 -u - "$@"
      - |
        import tensorflow as tf
        import numpy as np
        import argparse

        parser = argparse.ArgumentParser()
        parser.add_argument('--model', required=True)
        parser.add_argument('--pos_train_data', required=True)
        parser.add_argument('--neg_train_data', required=True)
        parser.add_argument('--trained_model', required=True)
        parser.add_argument('--num_epochs', type=int, required=True)
        parser.add_argument('--threshold', type=float, required=True)
        parser.add_argument('--learning_rate', type=float, required=True)
        args = parser.parse_args()

        # Load model
        model = tf.keras.models.load_model(args.model)

        # Load positive and negative datasets
        pos_ds = tf.data.experimental.load(args.pos_train_data)
        neg_ds = tf.data.experimental.load(args.neg_train_data)

        # Unbatch datasets
        pos_samples = list(tfds for tfds in pos_ds.unbatch().as_numpy_iterator())
        neg_samples = list(tfds for tfds in neg_ds.unbatch().as_numpy_iterator())

        x_pos = np.stack([x for x, _ in pos_samples]).astype(np.float32)
        x_neg = np.stack([x for x, _ in neg_samples]).astype(np.float32)

        class ForwardForwardTrainer:
            def __init__(self, model, threshold, learning_rate, num_epochs):
                self.model = model
                self.threshold = threshold
                self.learning_rate = learning_rate
                self.num_epochs = num_epochs

            def forward_forward_train(self, x_pos, x_neg):
                for i, layer in enumerate(self.model.layers):
                    # Skip layers with no trainable weights (e.g., Dropout)
                    if not layer.trainable_weights:
                        print(f"Skipping layer {i} - {layer.name} (no trainable weights)")
                        continue

                    print(f"\n--- Training layer {i} - {layer.name} ---")

                    # Partial model up to layer i (inclusive)
                    partial_model = tf.keras.Sequential(self.model.layers[:i+1])

                    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)

                    for epoch in range(self.num_epochs):
                        with tf.GradientTape() as tape:
                            # Forward pass positive and negative
                            act_pos = partial_model(x_pos)
                            act_neg = partial_model(x_neg)

                            # Goodness
                            g_pos = tf.reduce_mean(tf.reduce_sum(tf.square(act_pos), axis=1))
                            g_neg = tf.reduce_mean(tf.reduce_sum(tf.square(act_neg), axis=1))

                            # FF loss
                            loss = tf.math.log(1 + tf.exp(-g_pos + self.threshold)) + tf.math.log(1 + tf.exp(g_neg - self.threshold))

                        # Compute gradients only for current layer weights
                        grads = tape.gradient(loss, layer.trainable_weights)
                        optimizer.apply_gradients(zip(grads, layer.trainable_weights))

                        print(f"Layer {i+1}/{len(self.model.layers)} Epoch {epoch+1}/{self.num_epochs} Loss: {loss.numpy():.4f}")

        # Train model with FF
        trainer = ForwardForwardTrainer(
            model=model,
            threshold=args.threshold,
            learning_rate=args.learning_rate,
            num_epochs=args.num_epochs
        )
        trainer.forward_forward_train(x_pos, x_neg)

        # Save trained model
        model.save(args.trained_model)

    args:
      - --model
      - {inputPath: model}
      - --pos_train_data
      - {inputPath: pos_train_data}
      - --neg_train_data
      - {inputPath: neg_train_data}
      - --trained_model
      - {outputPath: trained_model}
      - --num_epochs
      - {inputValue: num_epochs}
      - --threshold
      - {inputValue: threshold}
      - --learning_rate
      - {inputValue: learning_rate}
